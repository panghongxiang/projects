{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb513576-b1bf-4e51-928f-2f1db73e683e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models, utils\n",
    "from gensim.models import LdaModel\n",
    "from gensim import similarities\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from cleantext import clean\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0078c81c-9ae5-4f31-a78b-fbfda099df50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABOUT HOPPER At Hopper we re on a mission to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At Noom we use scientifically proven methods t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decode M Data Science Manager Job Description ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sapphire Digital seeks a dynamic and driven mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Director Data Science 200537 Description Edelm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "0  ABOUT HOPPER At Hopper we re on a mission to m...\n",
       "1  At Noom we use scientifically proven methods t...\n",
       "2  Decode M Data Science Manager Job Description ...\n",
       "3  Sapphire Digital seeks a dynamic and driven mi...\n",
       "4  Director Data Science 200537 Description Edelm..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import data files\n",
    "jd_ds = pd.read_csv(\"../data/jd_ds.csv\",encoding='utf-8')\n",
    "jd_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f90ac2a-3d58-4e3e-8540-81dc667efe8c",
   "metadata": {},
   "source": [
    "### Training LDA model using words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5331e71-de91-436a-b144-d210b680eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = text.lower().split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87d5633d-1539-4fd2-bc62-0dacc1c8ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess job descriptions\n",
    "preprocessed_job_descriptions = jd_ds['description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cfca657-24a8-4ef3-a666-a2df5588e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary representation of job descriptions\n",
    "dictionary = corpora.Dictionary(preprocessed_job_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dec81e2-b990-4ba6-ac1b-bf74220ad452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(desc) for desc in preprocessed_job_descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57fd934-db80-4f9d-809e-aa683b7731ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model\n",
    "lda_model = models.LdaModel(doc_term_matrix, num_topics=10, id2word=dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f7a495-b179-4f94-be3d-dfa2106c5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and print top topics\n",
    "top_topics = lda_model.show_topics(num_topics=10, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c8aabf4-8a38-4172-9ea5-3702abce3270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.009*\"work\" + 0.008*\"employment\" + 0.008*\"information\" + 0.008*\"required\" + 0.008*\"research\" + 0.008*\"job\" + 0.007*\"employee\" + 0.006*\"position\" + 0.006*\"experience\" + 0.006*\"status\"')\n",
      "(1, '0.027*\"data\" + 0.015*\"experience\" + 0.013*\"team\" + 0.013*\"learning\" + 0.010*\"machine\" + 0.009*\"work\" + 0.007*\"science\" + 0.006*\"model\" + 0.006*\"company\" + 0.005*\"product\"')\n",
      "(2, '0.027*\"data\" + 0.013*\"experience\" + 0.010*\"work\" + 0.010*\"ability\" + 0.009*\"management\" + 0.009*\"skill\" + 0.008*\"business\" + 0.007*\"report\" + 0.007*\"project\" + 0.007*\"knowledge\"')\n",
      "(3, '0.022*\"business\" + 0.019*\"data\" + 0.015*\"experience\" + 0.010*\"solution\" + 0.008*\"project\" + 0.006*\"work\" + 0.006*\"year\" + 0.005*\"technology\" + 0.005*\"service\" + 0.004*\"team\"')\n",
      "(4, '0.022*\"data\" + 0.013*\"experience\" + 0.011*\"technology\" + 0.008*\"cloud\" + 0.007*\"client\" + 0.007*\"security\" + 0.007*\"platform\" + 0.007*\"team\" + 0.006*\"service\" + 0.006*\"working\"')\n",
      "(5, '0.014*\"experience\" + 0.011*\"development\" + 0.008*\"team\" + 0.007*\"work\" + 0.007*\"skill\" + 0.007*\"research\" + 0.006*\"cell\" + 0.006*\"clinical\" + 0.006*\"product\" + 0.006*\"scientist\"')\n",
      "(6, '0.017*\"student\" + 0.014*\"school\" + 0.011*\"johnson\" + 0.009*\"janssen\" + 0.009*\"data\" + 0.008*\"health\" + 0.008*\"course\" + 0.006*\"research\" + 0.006*\"penn\" + 0.006*\"harris\"')\n",
      "(7, '0.032*\"data\" + 0.013*\"business\" + 0.013*\"team\" + 0.013*\"experience\" + 0.013*\"analytics\" + 0.011*\"product\" + 0.011*\"analysis\" + 0.009*\"insight\" + 0.008*\"science\" + 0.005*\"model\"')\n",
      "(8, '0.064*\"data\" + 0.033*\"experience\" + 0.012*\"year\" + 0.010*\"business\" + 0.009*\"skill\" + 0.008*\"sql\" + 0.008*\"solution\" + 0.007*\"development\" + 0.007*\"tool\" + 0.007*\"team\"')\n",
      "(9, '0.017*\"ibm\" + 0.013*\"quantum\" + 0.013*\"data\" + 0.010*\"world\" + 0.010*\"business\" + 0.009*\"industry\" + 0.009*\"client\" + 0.009*\"experience\" + 0.008*\"work\" + 0.008*\"development\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in top_topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bcfa228-f6d6-4389-b316-ec21b8b17f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = []\n",
    "num_topics = 10\n",
    "for topic_id in range(num_topics):\n",
    "    top_words = lda_model.show_topic(topic_id, topn=10)  # Get the top 10 words for each topic\n",
    "    words = [word for word, _ in top_words]\n",
    "    topic_label = ', '.join(words)  # Join the words into a single string\n",
    "    topic_labels.append(topic_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "660b120d-164c-489b-9b97-394694668a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: work, employment, information, required, research, job, employee, position, experience, status\n",
      "Topic 1: data, experience, team, learning, machine, work, science, model, company, product\n",
      "Topic 2: data, experience, work, ability, management, skill, business, report, project, knowledge\n",
      "Topic 3: business, data, experience, solution, project, work, year, technology, service, team\n",
      "Topic 4: data, experience, technology, cloud, client, security, platform, team, service, working\n",
      "Topic 5: experience, development, team, work, skill, research, cell, clinical, product, scientist\n",
      "Topic 6: student, school, johnson, janssen, data, health, course, research, penn, harris\n",
      "Topic 7: data, business, team, experience, analytics, product, analysis, insight, science, model\n",
      "Topic 8: data, experience, year, business, skill, sql, solution, development, tool, team\n",
      "Topic 9: ibm, quantum, data, world, business, industry, client, experience, work, development\n"
     ]
    }
   ],
   "source": [
    "# Print the topic labels\n",
    "for i, label in enumerate(topic_labels):\n",
    "    print(f\"Topic {i}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d070591-43f0-4e64-9bfe-ee0fba595a52",
   "metadata": {},
   "source": [
    "### Training LDA model using bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61cc3aea-daa4-4293-94c7-e64ee50b060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_bigram(text):\n",
    "    # Tokenize text into sentences\n",
    "    sentences = [sent for sent in nltk.sent_tokenize(text)]\n",
    "    \n",
    "    # Tokenize sentences into words\n",
    "    tokenized_sentences = [utils.simple_preprocess(sent) for sent in sentences]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentences = [[token for token in sent if token not in stop_words] for sent in tokenized_sentences]\n",
    "    \n",
    "    # Create bigrams\n",
    "    bigram_model = Phrases(sentences, min_count=5, threshold=100)\n",
    "    bigram_phraser = Phraser(bigram_model)\n",
    "    bigram_sentences = [bigram_phraser[sent] for sent in sentences]\n",
    "    \n",
    "    # Convert bigrams into single words using '_'\n",
    "    for i in range(len(bigram_sentences)):\n",
    "        for j in range(len(bigram_sentences[i])):\n",
    "            if '_' in bigram_sentences[i][j]:\n",
    "                bigram_sentences[i][j] = bigram_sentences[i][j].replace('_', '')\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    preprocessed_sentences = [[lemmatizer.lemmatize(token) for token in sent] for sent in bigram_sentences]\n",
    "    \n",
    "    # Flatten sentences into a single list of words\n",
    "    words = [word for sent in preprocessed_sentences for word in sent]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e11277f6-f1f1-4f30-ac75-705cfbf80eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess job descriptions\n",
    "preprocessed_job_descriptions_bigram = jd_ds['description'].apply(preprocess_text_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a5ef89c-2a91-4e65-b7d7-516543cdfe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify bigrams and add them to the dictionary\n",
    "bigram_model = Phrases(preprocessed_job_descriptions_bigram, min_count=5, threshold=100)\n",
    "bigram_phraser = Phraser(bigram_model)\n",
    "\n",
    "for i in range(len(preprocessed_job_descriptions_bigram)):\n",
    "    for token in bigram_phraser[preprocessed_job_descriptions_bigram[i]]:\n",
    "        if '_' in token:\n",
    "            preprocessed_job_descriptions_bigram[i].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6171bbe-b1c6-4956-92f8-80171bd3511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary representation of job descriptions\n",
    "dictionary_bigram = corpora.Dictionary(preprocessed_job_descriptions_bigram.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3942c9a-8802-4395-b509-fbe760092307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-term matrix\n",
    "doc_term_matrix_bigram = [dictionary.doc2bow(desc) for desc in preprocessed_job_descriptions_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "445d87df-aa1c-4585-b67f-e91ed401cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model\n",
    "lda_model_bigram = models.LdaModel(doc_term_matrix_bigram, num_topics=10, id2word=dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0ac6ceb-c217-4dc0-892a-be66260ba426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.009*\"work\" + 0.008*\"employment\" + 0.008*\"information\" + 0.008*\"required\" + 0.008*\"research\" + 0.008*\"job\" + 0.007*\"employee\" + 0.006*\"position\" + 0.006*\"experience\" + 0.006*\"status\"')\n",
      "(1, '0.027*\"data\" + 0.015*\"experience\" + 0.013*\"team\" + 0.013*\"learning\" + 0.010*\"machine\" + 0.009*\"work\" + 0.007*\"science\" + 0.006*\"model\" + 0.006*\"company\" + 0.005*\"product\"')\n",
      "(2, '0.027*\"data\" + 0.013*\"experience\" + 0.010*\"work\" + 0.010*\"ability\" + 0.009*\"management\" + 0.009*\"skill\" + 0.008*\"business\" + 0.007*\"report\" + 0.007*\"project\" + 0.007*\"knowledge\"')\n",
      "(3, '0.022*\"business\" + 0.019*\"data\" + 0.015*\"experience\" + 0.010*\"solution\" + 0.008*\"project\" + 0.006*\"work\" + 0.006*\"year\" + 0.005*\"technology\" + 0.005*\"service\" + 0.004*\"team\"')\n",
      "(4, '0.022*\"data\" + 0.013*\"experience\" + 0.011*\"technology\" + 0.008*\"cloud\" + 0.007*\"client\" + 0.007*\"security\" + 0.007*\"platform\" + 0.007*\"team\" + 0.006*\"service\" + 0.006*\"working\"')\n",
      "(5, '0.014*\"experience\" + 0.011*\"development\" + 0.008*\"team\" + 0.007*\"work\" + 0.007*\"skill\" + 0.007*\"research\" + 0.006*\"cell\" + 0.006*\"clinical\" + 0.006*\"product\" + 0.006*\"scientist\"')\n",
      "(6, '0.017*\"student\" + 0.014*\"school\" + 0.011*\"johnson\" + 0.009*\"janssen\" + 0.009*\"data\" + 0.008*\"health\" + 0.008*\"course\" + 0.006*\"research\" + 0.006*\"penn\" + 0.006*\"harris\"')\n",
      "(7, '0.032*\"data\" + 0.013*\"business\" + 0.013*\"team\" + 0.013*\"experience\" + 0.013*\"analytics\" + 0.011*\"product\" + 0.011*\"analysis\" + 0.009*\"insight\" + 0.008*\"science\" + 0.005*\"model\"')\n",
      "(8, '0.064*\"data\" + 0.033*\"experience\" + 0.012*\"year\" + 0.010*\"business\" + 0.009*\"skill\" + 0.008*\"sql\" + 0.008*\"solution\" + 0.007*\"development\" + 0.007*\"tool\" + 0.007*\"team\"')\n",
      "(9, '0.017*\"ibm\" + 0.013*\"quantum\" + 0.013*\"data\" + 0.010*\"world\" + 0.010*\"business\" + 0.009*\"industry\" + 0.009*\"client\" + 0.009*\"experience\" + 0.008*\"work\" + 0.008*\"development\"')\n"
     ]
    }
   ],
   "source": [
    "# Extract and print top topics\n",
    "top_topics_bigram = lda_model.show_topics(num_topics=10, num_words=10)\n",
    "\n",
    "for topic in top_topics_bigram:\n",
    "    print(topic)# Extract and print top topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a2baac4-bef7-41c8-8714-1f338f37b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained LDA model\n",
    "lda_model.save(\"../model/lda_model_ds\")\n",
    "lda_model_bigram.save(\"../model/lda_model_ds_bigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e0d03-0464-4ee0-be4b-e9340d15b83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384b5f3-6e9e-4b94-97ab-b3a868ea104e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f69b0f4-0541-4723-be06-4a41f44c883c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cded1-340d-40c7-b880-a20b83ef5d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b560c6-6f4e-4fe4-9546-a0c696526985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a3dcc-b6f3-45fe-ac2b-c66a90d24ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi-sg",
   "language": "python",
   "name": "dsi-sg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
